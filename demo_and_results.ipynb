{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Colab Init\n",
    "\n",
    "Don't need to run if you cloned the repo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2301b744f0846fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!rm -rf sample_data\n",
    "\n",
    "!wget https://raw.githubusercontent.com/MeepOwned13/dnn_el_load/main/data/country_data.csv -P data/\n",
    "!wget https://raw.githubusercontent.com/MeepOwned13/dnn_el_load/main/models/trainer_lib.py -P models/\n",
    "!wget https://raw.githubusercontent.com/MeepOwned13/dnn_el_load/main/models/torch_model_definitions.py -P models/\n",
    "!wget https://raw.githubusercontent.com/MeepOwned13/dnn_el_load/main/models/params.py -P models/\n",
    "\n",
    "!npx degit github:MeepOwned13/dnn_el_load/final_eval_results final_eval_results\n",
    "\n",
    "!python -m pip install overrides"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f22cd82e593aabf0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec8f5cf41ecd41f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import models.torch_model_definitions as tmd\n",
    "import models.trainer_lib as tl\n",
    "from models.params import PARAMS\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "results = pd.read_csv('final_eval_results/results.csv')\n",
    "\n",
    "MODEL_NAMES = {\n",
    "    'reg_s2s': 'Regular Seq2Seq',\n",
    "    'att_s2s': 'Attention Seq2Seq',\n",
    "    'pos_att_s2s': 'Concatenated Positional Encoding & Attention',\n",
    "    'add_pos_att_s2s': 'Additive Positional Encoding & Attention',\n",
    "}\n",
    "\n",
    "# Getting all available pretrained models\n",
    "pts = [f[:-3] for f in os.listdir('final_eval_results') if f[-3:] == '.pt']\n",
    "regex = re.compile(r'(\\d{2})_(\\d{1,2})_(\\w*)')\n",
    "\n",
    "available_24 = pd.DataFrame(columns=results['Model'].unique(),\n",
    "                            index=results['Prediction length'].unique(),\n",
    "                            dtype=str).fillna(\"\\u2a2f\")\n",
    "available_48 = available_24.copy()\n",
    "\n",
    "for f in pts:\n",
    "    match = regex.match(f)\n",
    "    \n",
    "    if match.group(1) == '24':\n",
    "        available_24[match.group(3)][int(match.group(2))] = \"\\u2714\"\n",
    "    elif match.group(1) == '48':\n",
    "        available_48[match.group(3)][int(match.group(2))] = \"\\u2714\"\n",
    "\n",
    "print(f\"Pretrained models available for 24 hour lookback (Model - Prediction length):\\n{available_24.to_string(justify='center')}\")\n",
    "\n",
    "print(f\"\\nPretrained models available for 48 hour lookback (Model - Prediction length):\\n{available_48.to_string(justify='center')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choose a model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b1d35777c4caa2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep in mind when loading: this section always returns the model that performed best from the given runs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed353410460a8a66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the model right now?, make sure to change runtime type to GPU!\n",
    "TRAIN_MODEL = False\n",
    "CUSTOM_EPOCHS = 10 # set to None for default\n",
    "# choice of model, sequence and lookback in the following format:\n",
    "    # \"<sequence_length>_<lookback_length>_<model>\" [e.g. \"24_6_reg_s2s\"]\n",
    "CHOICE = \"48_48_pos_att_s2s\"\n",
    "\n",
    "### Above are configurable parameters ###\n",
    "if CHOICE not in pts:\n",
    "    raise Exception(f\"Choice {CHOICE} not available, check 'Imports and Setup' cell output\")\n",
    "\n",
    "if TRAIN_MODEL and tl.TRAINER_LIB_DEVICE == torch.device(\"cpu\"):\n",
    "    print(\"WARNING: Using CPU for training might make it take a long time, use CUDA instead if possible\")\n",
    "\n",
    "# Setting up params which are not predefined\n",
    "match = regex.match(CHOICE)\n",
    "params = deepcopy(PARAMS[match.group(3)])\n",
    "params['seq_len'] = int(match.group(1))\n",
    "params['pred_len'] = int(match.group(2))\n",
    "params['model_params']['pred_len'] = int(match.group(2))\n",
    "# Models were trained with a +2 embedding size on 48 hour sequence length\n",
    "if params['seq_len'] > 24:\n",
    "    params['model_params']['embedding_size'] += 2\n",
    "\n",
    "# Print info\n",
    "print(f\"{'Training' if TRAIN_MODEL else 'Loading'} {MODEL_NAMES[match.group(3)]} model, \"\n",
    "      f\"using {params['seq_len']} sequence length (lookback) and predicting {params['pred_len']} hour(s) ahead.\")\n",
    "\n",
    "# Load data\n",
    "dataset = tl.load_country_wide_dataset('data/country_data.csv', until='2019-12-31 23:00:00')\n",
    "X = dataset.to_numpy(dtype=np.float32)\n",
    "y = dataset['el_load'].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Define splits (predetermined, using 2/3 for training, 1/3 for testing\n",
    "# No cross validation performed to allow more models and configs for testing\n",
    "split_len = len(X) // 3\n",
    "train_val_sp: int = split_len * 2 - split_len // 8\n",
    "val_test_sp: int = split_len * 2\n",
    "x_train, x_val, x_test = X[:train_val_sp], X[train_val_sp:val_test_sp], X[val_test_sp:]\n",
    "y_train, y_val, y_test = y[:train_val_sp], y[train_val_sp:val_test_sp], y[val_test_sp:]\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    params['epochs'] = CUSTOM_EPOCHS or params['epochs']\n",
    "    \n",
    "    wrapper = tl.S2STSWrapper(params['model'](**params['model_params']).to(tl.TRAINER_LIB_DEVICE), params['seq_len'], params['pred_len'])\n",
    "    result = wrapper.train_strategy(x_train, y_train, x_val, y_val, x_test, y_test, **params)\n",
    "    \n",
    "    y_pred, y_true = wrapper.predict_for_comparison(x_test, y_test)\n",
    "    tl.TSMWrapper.plot_losses([result[0]], [result[1]], [result[2]])\n",
    "else:\n",
    "    path = f\"final_eval_results/{CHOICE}.pt\"\n",
    "    wrapper = tl.S2STSWrapper(params['model'](**params['model_params']).to(tl.TRAINER_LIB_DEVICE), params['seq_len'], params['pred_len'])\n",
    "    \n",
    "    wrapper.load_state(path)\n",
    "    y_pred, y_true = wrapper.predict_for_comparison(x_test, y_test)\n",
    "\n",
    "tl.TSMWrapper.print_evaluation_info(y_pred, y_true, 0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "add1e3b91cc1b22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5006b1cb511dc01c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Index of entry to plot, None means random\n",
    "PLOT_START = None\n",
    "\n",
    "### Above are configurable parameters ###\n",
    "\n",
    "if PLOT_START is None:\n",
    "    PLOT_START = random.randint(0, y_true.shape[0] - 1)\n",
    "    \n",
    "if PLOT_START > y_true.shape[0] - 1:\n",
    "    raise ValueError(f\"Index {PLOT_START} doesn't exist, maximum is {y_true.shape[0] - 1}\")\n",
    "\n",
    "plt.plot(y_true[PLOT_START], label=\"True\", color=\"green\")\n",
    "plt.plot(y_pred[PLOT_START], label=\"Prediction\", color=\"red\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f88ae11ef0e94ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overall performance comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de55b293eb83c503"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means = results.groupby(['Sequence length', 'Prediction length', 'Model']).mean()\n",
    "stds = results.groupby(['Sequence length', 'Prediction length', 'Model']).std()\n",
    "\n",
    "means['Train Time'], stds['Train Time'] = means['Train Time'] / 60, stds['Train Time'] / 60\n",
    "means['Pred Time'], stds['Pred Time'] = means['Pred Time'] * 1000, stds['Pred Time'] * 1000\n",
    "\n",
    "rounding = {'MAE': 2, 'MSE': 1, 'RMSE': 2, 'MAPE': 4, 'MPE': 4, 'Train Time': 2, 'Pred Time': 0}\n",
    "means, stds = means.round(rounding), stds.round(rounding)\n",
    "\n",
    "mean_and_std = pd.DataFrame(columns=[], dtype=str)\n",
    "\n",
    "for col in means.columns:\n",
    "    mean_and_std[col] = means[col].astype(str) + \"\\u00b1\" + stds[col].astype(str)\n",
    "\n",
    "print(f'Train Time is in minutes, Pred Time is in milliseconds (calculated for predicting the entire test set)')\n",
    "mean_and_std"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85b5876e2361e3dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparison graphs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf8431ca01fa60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_per_model(data: pd.DataFrame, ylabel: str, axs=None, title=None):\n",
    "    \"\"\"Make sure to call plt.show() after figure is done.\"\"\"\n",
    "    if axs is None:\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xticks([6, 12, 24, 48])\n",
    "        plt.grid()\n",
    "        axs = plt\n",
    "    else:\n",
    "        axs.set_title(title)\n",
    "        axs.set_xlabel('Hour')\n",
    "        axs.set_ylabel(ylabel)\n",
    "        axs.set_xticks([6, 12, 24, 48])\n",
    "        axs.grid()\n",
    "    \n",
    "    for m in data['Model'].unique():\n",
    "        per_model = data[data['Model'] == m].drop('Model', axis=1).to_numpy()\n",
    "        x, y = per_model[:, 0], per_model[:, 1]\n",
    "        axs.plot(x, y, marker='o', linewidth=1, linestyle='dashed', label=MODEL_NAMES.get(m, m))\n",
    "        \n",
    "        axs.legend()\n",
    "    \n",
    "def plot_for_len(data: pd.DataFrame, seq_len=24, to_plot: str = 'RMSE'):\n",
    "    if to_plot not in data.drop(['Model', 'Sequence length', 'Prediction length'], axis=1, errors=\"ignore\").columns:\n",
    "        raise ValueError(f\"Can't plot {to_plot}, it doesn't exist in the DataFrame\")\n",
    "    \n",
    "    gp = data.groupby(['Model', 'Sequence length', 'Prediction length'], as_index=False)[['Model', to_plot]]\n",
    "\n",
    "    gmean = gp.mean([to_plot])\n",
    "    gmin = gp.min([to_plot])\n",
    "    \n",
    "    gmean = gmean[gmean['Sequence length'] == seq_len].drop('Sequence length', axis=1)\n",
    "    gmin = gmin[gmin['Sequence length'] == seq_len].drop('Sequence length', axis=1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    plot_per_model(gmean, to_plot, axs[0], f'AVG for {seq_len} sequence')\n",
    "    plot_per_model(gmin, to_plot, axs[1], f'MIN for {seq_len} sequence')\n",
    "    plt.show()\n",
    "\n",
    "plot_for_len(results, 24, 'RMSE')\n",
    "plot_for_len(results, 48, 'RMSE')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26ca54456c88aed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
