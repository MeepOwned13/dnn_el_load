{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e8f02c4531e07f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-09T15:35:51.082920100Z",
     "start_time": "2023-12-09T15:35:47.304555900Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import trainer_lib as tl\n",
    "import torch_model_definitions as tmd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c62b93b0a0d0b14"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df: pd.DataFrame = tl.load_country_wide_dataset('../data/country_data.csv', until='2019-12-31 23:00:00')\n",
    "# I'll limit the amount of data to make it run faster\n",
    "# Models generally performed the best on this part of the dataset in my TDK\n",
    "\n",
    "X = df.to_numpy(dtype=np.float32)\n",
    "y = df['el_load'].to_numpy(dtype=np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T15:35:51.277645100Z",
     "start_time": "2023-12-09T15:35:51.085922800Z"
    }
   },
   "id": "cc19b65985e4e476"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grid search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7697c4ccd76de55a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seq2seq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf5484ce609a5485"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid search 001] BEGIN - params: {'epochs': 1000, 'lr': 0.001, 'model': <class 'torch_model_definitions.Seq2seq'>, 'embedding_size': 10, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5, 'out_noise': 0.05, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 449: train loss: 0.035275, val loss: 0.059079, test loss: 0.074390\n",
      "[Fold 2] END - RMSE loss: 167.899 - Time: 6.1 min.\n",
      "[Grid search 001] END - Score: 167.89887791 * \n",
      "\n",
      "Best params: {'epochs': 1000, 'lr': 0.001, 'model': <class 'torch_model_definitions.Seq2seq'>, 'embedding_size': 10, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5, 'out_noise': 0.05, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "Best score: 167.89887790907062\n"
     ]
    }
   ],
   "source": [
    "# this is the setup that performed the best in my TDK with 24 seq_len and 3 pred_len, I won't grid search it further, it seems to perfrom well\n",
    "grid = tl.Grid({\n",
    "    'epochs': [2000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [tmd.Seq2seq],\n",
    "    'embedding_size': [10],\n",
    "    'num_layers': [1],\n",
    "    'bidirectional': [True],\n",
    "    'dropout': [0.5],\n",
    "    'out_noise': [0.05],\n",
    "    'batch_size': [2048],\n",
    "    'pred_len': [12],\n",
    "    'es_p': [20]\n",
    "}) # n_splits defaulted to 2, val_mod to 8\n",
    "\n",
    "wrapper = tl.S2STSWrapper(tmd.Seq2seq(), seq_len=24, pred_len=12, teacher_forcing_decay=0.01)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T16:19:24.132535200Z",
     "start_time": "2023-11-27T16:13:17.384299700Z"
    }
   },
   "id": "6eab3e2a01ebe19b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AttentionSeq2seq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d8a02e4812e87c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid search 001] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 8, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 1148: train loss: 0.029809, val loss: 0.052003, test loss: 0.072072\n",
      "[Fold 2] END - RMSE loss: 157.203 - Time: 13.2 min.\n",
      "[Grid search 001] END - Score: 157.20342862 * \n",
      "[Grid search 002] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 10, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 302: train loss: 0.036287, val loss: 0.064952, test loss: 0.081563\n",
      "[Fold 2] END - RMSE loss: 176.641 - Time: 3.5 min.\n",
      "[Grid search 002] END - Score: 176.64097813 \n",
      "[Grid search 003] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 12, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 400: train loss: 0.026630, val loss: 0.050804, test loss: 0.062934\n",
      "[Fold 2] END - RMSE loss: 160.596 - Time: 4.2 min.\n",
      "[Grid search 003] END - Score: 160.59552259 \n",
      "[Grid search 004] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 8, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 509: train loss: 0.027498, val loss: 0.056432, test loss: 0.070802\n",
      "[Fold 2] END - RMSE loss: 176.041 - Time: 5.8 min.\n",
      "[Grid search 004] END - Score: 176.04087216 \n",
      "[Grid search 005] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 419: train loss: 0.023964, val loss: 0.048452, test loss: 0.058557\n",
      "[Fold 2] END - RMSE loss: 153.717 - Time: 6.3 min.\n",
      "[Grid search 005] END - Score: 153.71650423 * \n",
      "[Grid search 006] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 12, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 188: train loss: 0.030094, val loss: 0.054724, test loss: 0.084769\n",
      "[Fold 2] END - RMSE loss: 174.687 - Time: 2.9 min.\n",
      "[Grid search 006] END - Score: 174.68685711 \n",
      "\n",
      "Best params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 20}\n",
      "Best score: 153.7165042273438\n"
     ]
    }
   ],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [2000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.002],\n",
    "    'model': [tmd.AttentionSeq2seq],\n",
    "    'embedding_size': [8, 10, 12],\n",
    "    'bidirectional': [False, True],\n",
    "    'dropout': [0.0],  # set to 0, the model wasn't showing major signs of overfitting\n",
    "    'out_noise': [0.00],  # set to 0, the model wasn't showing major signs of overfitting\n",
    "    'batch_size': [2048],\n",
    "    'pred_len': [12],\n",
    "    'es_p': [20]\n",
    "}) # n_splits defaulted to 2, val_mod to 8\n",
    "\n",
    "wrapper = tl.S2STSWrapper(tmd.AttentionSeq2seq(), seq_len=24, pred_len=12, teacher_forcing_decay=0.01)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:24:56.961987800Z",
     "start_time": "2023-11-28T12:48:50.599461600Z"
    }
   },
   "id": "eca10e09ccebbe3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention and positional encoding seq2seq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b497b491df8c1d7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid search 001] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 10, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 1081: train loss: 0.028514, val loss: 0.046966, test loss: 0.060109\n",
      "[Fold 2] END - RMSE loss: 154.056 - Time: 12.0 min.\n",
      "[Grid search 001] END - Score: 154.05562231 * \n",
      "[Grid search 002] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 12, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 1143: train loss: 0.022216, val loss: 0.043605, test loss: 0.058386\n",
      "[Fold 2] END - RMSE loss: 156.615 - Time: 12.9 min.\n",
      "[Grid search 002] END - Score: 156.61451855 \n",
      "[Grid search 003] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 14, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 916: train loss: 0.024251, val loss: 0.047231, test loss: 0.069817\n",
      "[Fold 2] END - RMSE loss: 163.948 - Time: 11.6 min.\n",
      "[Grid search 003] END - Score: 163.94836506 \n",
      "[Grid search 004] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 660: train loss: 0.024047, val loss: 0.051722, test loss: 0.060806\n",
      "[Fold 2] END - RMSE loss: 155.825 - Time: 10.3 min.\n",
      "[Grid search 004] END - Score: 155.82526647 \n",
      "[Grid search 005] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 12, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 375: train loss: 0.030664, val loss: 0.055114, test loss: 0.060935\n",
      "[Fold 2] END - RMSE loss: 159.953 - Time: 5.7 min.\n",
      "[Grid search 005] END - Score: 159.95272129 \n",
      "[Grid search 006] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 14, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 289: train loss: 0.029928, val loss: 0.057303, test loss: 0.081287\n",
      "[Fold 2] END - RMSE loss: 174.877 - Time: 4.4 min.\n",
      "[Grid search 006] END - Score: 174.87709970 \n",
      "\n",
      "Best params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 10, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "Best score: 154.05562231098546\n"
     ]
    }
   ],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [2000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [tmd.PosAttSeq2seq],\n",
    "    'embedding_size': [10, 12, 14],\n",
    "    'bidirectional': [False, True],\n",
    "    'dropout': [0.0],\n",
    "    'out_noise': [0.0],\n",
    "    'batch_size': [2048],\n",
    "    'pred_len': [12],\n",
    "    'es_p': [15]\n",
    "}) # n_splits defaulted to 2, val_mod to 8\n",
    "\n",
    "wrapper = tl.S2STSWrapper(tmd.PosAttSeq2seq(), seq_len=24, pred_len=12, teacher_forcing_decay=0.01)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:22:25.493426100Z",
     "start_time": "2023-11-28T13:25:28.472943600Z"
    }
   },
   "id": "3fbb0acbb2d1cdc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Direct comparison between PE and non-PE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2195121ec5299d14"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid search 001] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 10, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 1123: train loss: 0.025884, val loss: 0.052400, test loss: 0.058205\n",
      "[Fold 2] END - RMSE loss: 157.440 - Time: 12.0 min.\n",
      "[Grid search 001] END - Score: 157.43972061 * \n",
      "[Grid search 002] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 10, 'bidirectional': False, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 1956: train loss: 0.020990, val loss: 0.043597, test loss: 0.049572\n",
      "[Fold 2] END - RMSE loss: 151.122 - Time: 21.8 min.\n",
      "[Grid search 002] END - Score: 151.12160098 * \n",
      "[Grid search 003] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 312: train loss: 0.036610, val loss: 0.064422, test loss: 0.072786\n",
      "[Fold 2] END - RMSE loss: 174.258 - Time: 4.7 min.\n",
      "[Grid search 003] END - Score: 174.25843999 \n",
      "[Grid search 004] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 797: train loss: 0.020590, val loss: 0.044562, test loss: 0.056657\n",
      "[Fold 2] END - RMSE loss: 150.845 - Time: 12.6 min.\n",
      "[Grid search 004] END - Score: 150.84451069 * \n",
      "\n",
      "Best params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "Best score: 150.84451069313062\n"
     ]
    }
   ],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [2000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [tmd.AttentionSeq2seq, tmd.PosAttSeq2seq],\n",
    "    'embedding_size': [10],\n",
    "    'bidirectional': [False, True],\n",
    "    'dropout': [0.0],\n",
    "    'out_noise': [0.0],\n",
    "    'batch_size': [2048],\n",
    "    'pred_len': [12],\n",
    "    'es_p': [15]\n",
    "}) # n_splits defaulted to 2, val_mod to 8\n",
    "\n",
    "wrapper = tl.S2STSWrapper(tmd.AttentionSeq2seq(), seq_len=24, pred_len=12, teacher_forcing_decay=0.01)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:32:12.309172800Z",
     "start_time": "2023-11-28T14:41:05.770222Z"
    }
   },
   "id": "2d3bbfe037a3846d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bidirectional models perform better in this case too, but the difference for the regular attention model is small.\n",
    "Positional encoding might not be required because the input already has time components, but further testing is required."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f097b26dc893ab6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Longer sequence length"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25fe971458750d4b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid search 001] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.Seq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 617: train loss: 0.028289, val loss: 0.052054, test loss: 0.088466\n",
      "[Fold 2] END - RMSE loss: 176.173 - Time: 7.8 min.\n",
      "[Grid search 001] END - Score: 176.17316588 * \n",
      "[Grid search 002] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 429: train loss: 0.032189, val loss: 0.060340, test loss: 0.075414\n",
      "[Fold 2] END - RMSE loss: 175.832 - Time: 5.9 min.\n",
      "[Grid search 002] END - Score: 175.83212319 * \n",
      "[Grid search 003] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.Seq2seq'>, 'embedding_size': 12, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 622: train loss: 0.024899, val loss: 0.049034, test loss: 0.055763\n",
      "[Fold 2] END - RMSE loss: 154.561 - Time: 8.0 min.\n",
      "[Grid search 003] END - Score: 154.56088884 * \n",
      "[Grid search 004] BEGIN - params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.PosAttSeq2seq'>, 'embedding_size': 12, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 327: train loss: 0.033156, val loss: 0.060243, test loss: 0.088511\n",
      "[Fold 2] END - RMSE loss: 171.068 - Time: 4.7 min.\n",
      "[Grid search 004] END - Score: 171.06774060 \n",
      "\n",
      "Best params: {'epochs': 2000, 'lr': 0.001, 'model': <class 'torch_model_definitions.Seq2seq'>, 'embedding_size': 12, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "Best score: 154.56088884117807\n"
     ]
    }
   ],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [2000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [tmd.Seq2seq, tmd.PosAttSeq2seq],\n",
    "    'embedding_size': [10, 12],\n",
    "    'bidirectional': [True],\n",
    "    'dropout': [0.0],\n",
    "    'out_noise': [0.0],\n",
    "    'batch_size': [2048],\n",
    "    'pred_len': [12],\n",
    "    'es_p': [15]\n",
    "}) # n_splits defaulted to 2, val_mod to 8\n",
    "\n",
    "wrapper = tl.S2STSWrapper(tmd.Seq2seq(), seq_len=48, pred_len=12, teacher_forcing_decay=0.01)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T16:23:56.539547800Z",
     "start_time": "2023-12-09T15:57:31.852826800Z"
    }
   },
   "id": "95734ca8e70031d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This particular run produced interesting results, but the higher embedding size seems to help in longer sequence lengths."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "613b60d139d97a7c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grid search 001] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 10, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 166: train loss: 0.038162, val loss: 0.069109, test loss: 0.076788\n",
      "[Fold 2] END - RMSE loss: 184.082 - Time: 2.5 min.\n",
      "[Grid search 001] END - Score: 184.08244799 * \n",
      "[Grid search 002] BEGIN - params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 12, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "[Fold 2] BEGIN\n",
      "Early stopping... Epoch 285: train loss: 0.027887, val loss: 0.054195, test loss: 0.079248\n",
      "[Fold 2] END - RMSE loss: 165.895 - Time: 4.7 min.\n",
      "[Grid search 002] END - Score: 165.89517469 * \n",
      "\n",
      "Best params: {'epochs': 2000, 'lr': 0.002, 'model': <class 'torch_model_definitions.AttentionSeq2seq'>, 'embedding_size': 12, 'bidirectional': True, 'dropout': 0.0, 'out_noise': 0.0, 'batch_size': 2048, 'pred_len': 12, 'es_p': 15}\n",
      "Best score: 165.89517468683349\n"
     ]
    }
   ],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [2000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.002],\n",
    "    'model': [tmd.AttentionSeq2seq],\n",
    "    'embedding_size': [10, 12],\n",
    "    'bidirectional': [True],\n",
    "    'dropout': [0.0],\n",
    "    'out_noise': [0.0],\n",
    "    'batch_size': [2048],\n",
    "    'pred_len': [12],\n",
    "    'es_p': [15]\n",
    "}) # n_splits defaulted to 2, val_mod to 8\n",
    "\n",
    "wrapper = tl.S2STSWrapper(tmd.Seq2seq(), seq_len=48, pred_len=12, teacher_forcing_decay=0.01)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-09T17:05:56.986163700Z"
    }
   },
   "id": "f2e9baacf0a40f3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
